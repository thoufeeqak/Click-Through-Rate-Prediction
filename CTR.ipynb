{"cells":[{"cell_type":"markdown","source":["##  Featurize categorical data using one-hot-encoding"],"metadata":{}},{"cell_type":"code","source":["sqlContext.setConf('spark.sql.shuffle.partitions', '6') "],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import numpy as np\nfrom pyspark.mllib.linalg import SparseVector"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n\n    Note:\n        You should ensure that the indices used to create a SparseVector are sorted.\n\n    Args:\n        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n            (featureID, value) to unique integer.\n        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n            value).\n\n    Returns:\n        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n            identifiers for the (featureID, value) combinations that occur in the observation and\n            with values equal to 1.0.\n    \"\"\"\n    y=[]\n    for x in raw_feats:\n      y.append(ohe_dict_broadcast.value[x])\n      #print y\n    #print ohe_dict_broadcast.value[raw_feats]\n    #print np.ones(len(raw_feats))\n    y.sort()\n    return SparseVector(num_ohe_feats,y,np.ones(len(raw_feats)))\n\n# Calculate the number of features in sample_ohe_dict_manual\nnum_sample_ohe_feats = len(sample_ohe_dict_manual)\nsample_ohe_dict_manual_broadcast = sc.broadcast(sample_ohe_dict_manual)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.mllib.linalg import VectorUDT\n\ndef ohe_udf_generator(ohe_dict_broadcast):\n    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n\n    Note:\n        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n        the number of features before calling the one_hot_encoding function.\n\n    Args:\n        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n            (featureID, value) to unique integer.\n\n    Returns:\n        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n            function on each row in a given column.  This UDF should call the one_hot_encoding\n            function with the appropriate parameters.\n    \"\"\"\n    \n    length =len(ohe_dict_broadcast.value)\n    return udf(lambda x: one_hot_encoding(x,ohe_dict_broadcast,length), VectorUDT())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Automated creation of an OHE dictionary"],"metadata":{}},{"cell_type":"code","source":["def create_one_hot_dict(input_df):\n    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n\n    Args:\n        input_df (DataFrame with 'features' column): A DataFrame where each row contains a list of\n            (featureID, value) tuples.\n\n    Returns:\n        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n            unique integers.\n    \"\"\"\n    return (input_df\n              .select(explode(input_df.features))\n              .distinct()\n              .rdd\n              .map(lambda l:tuple(l[0]))\n              .zipWithIndex()\n              .collectAsMap()\n            )"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["##  Parse CTR data and generate OHE features"],"metadata":{}},{"cell_type":"code","source":["def cleanup_old_downloads():\n  from fnmatch import fnmatch\n  for f in dbutils.fs.ls('/tmp'):\n    name = str(f.name)\n    if fnmatch(name, 'criteo_*'):\n      dbutils.fs.rm(str(f.path), recurse=True)\n\ndef download_criteo(url):\n  from io import BytesIO\n  import urllib2\n  import tarfile\n  import uuid\n  import tempfile\n  import random\n  import string\n  import os\n\n  if not url.endswith('dac_sample.tar.gz'):\n    raise Exception('Check your download URL. Are you downloading the sample dataset?')\n\n  cleanup_old_downloads()\n\n\n  rng = random.SystemRandom()\n  tlds = ('.org', '.net', '.com', '.info', '.biz')\n  random_domain_name = (\n    ''.join(rng.choice(string.letters + string.digits) for i in range(64)) +\n    rng.choice(tlds)\n  )\n  random_id = str(uuid.uuid3(uuid.NAMESPACE_DNS, random_domain_name)).replace('-', '_')\n  unique_id = str(uuid.uuid3(uuid.NAMESPACE_DNS, random_id)).replace('-', '_')\n  dbfs_dir  = 'dbfs:/tmp/criteo_{0}'.format(unique_id)\n  dbfs_path = '{0}/data.txt'.format(dbfs_dir)\n  dbutils.fs.mkdirs(dbfs_dir)\n\n  \n  tmp = BytesIO()\n  req = urllib2.Request(url, headers={'User-Agent': 'Databricks'})\n  url_handle = urllib2.urlopen(req)\n  tmp.write(url_handle.read())\n  tmp.seek(0)\n  tf = tarfile.open(fileobj=tmp)\n  dac_sample = tf.extractfile('dac_sample.txt')\n  dac_sample = '\\n'.join([unicode(x.replace('\\n', '').replace('\\t', ',')) for x in dac_sample])\n\n  with tempfile.NamedTemporaryFile(mode='wb', delete=False, prefix='dac', suffix='.txt') as t:\n    t.write(dac_sample)\n    t.close()\n    dbutils.fs.cp('file://{0}'.format(t.name), dbfs_path)\n    os.unlink(t.name)\n\n  return dbfs_path"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["criteo_url = 'http://criteolabs.wpengine.com/wp-content/uploads/2015/04/dac_sample.tar.gz'"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["if ('downloaded_data_file' not in locals()) or (downloaded_data_file is None):\n  downloaded_data_file = download_criteo(criteo_url)\n\nif ('raw_df' in locals()) and (raw_df is not None):\n  print \"raw_df is already loaded. Nothing to do. (Set raw_df=None to reload it, then re-run this cell.)\"\nelse:\n  raw_df = sqlContext.read.text(downloaded_data_file).withColumnRenamed(\"value\", \"text\")\n\nprint \"raw_df initialized to read from {0}\".format(downloaded_data_file)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["  raw_df.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["###  Loading and splitting the data"],"metadata":{}},{"cell_type":"code","source":["weights = [.8, .1, .1]\nseed = 42\n\nraw_train_df, raw_validation_df, raw_test_df = raw_df.randomSplit(weights,seed)\n\nn_train = raw_train_df.cache().count()\nn_val = raw_validation_df.cache().count()\nn_test = raw_test_df.cache().count()\nprint n_train, n_val, n_test, n_train + n_val + n_test\nraw_df.show(1,truncate=False)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["###  Extract features"],"metadata":{}},{"cell_type":"code","source":["def parse_point(point):\n    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n\n    Note:\n        featureIDs should start at 0 and increase to the number of features - 1.\n\n    Args:\n        point (str): A comma separated string where the first value is the label and the rest\n            are features.\n\n    Returns:\n        list: A list of (featureID, value) tuples.\n    \"\"\"\n    y= point.split(',')\n    return (list(enumerate(y[1:])))\n\nprint parse_point(raw_df.select('text').first()[0])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import udf, split\nfrom pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType, DoubleType\n\nparse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),\n                                                         StructField('_2', StringType())])))\n\ndef parse_raw_df(raw_df):\n    \"\"\"Convert a DataFrame consisting of rows of comma separated text into labels and feature.\n\n\n    Args:\n        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n\n    Returns:\n        DataFrame: A DataFrame with 'label' and 'feature' columns.\n    \"\"\"\n    return raw_df.select(split(raw_df.text,',').getItem(0).cast(DoubleType()).alias('label'),parse_point_udf(raw_df.text).alias('feature')).cache()\n    \n    \n\nparsed_train_df = parse_raw_df(raw_train_df)\n\nfrom pyspark.sql.functions import (explode, col)\nnum_categories = (parsed_train_df\n                    .select(explode('feature').alias('feature'))\n                    .distinct()\n                    .select(col('feature').getField('_1').alias('featureNumber'))\n                    .groupBy('featureNumber')\n                    .sum()\n                    .orderBy('featureNumber')\n                    .collect())\n\nprint num_categories[2][1]"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["###  Create an OHE dictionary from the dataset"],"metadata":{}},{"cell_type":"code","source":["ctr_ohe_dict = create_one_hot_dict(parsed_train_df.select(parsed_train_df.feature.alias('features')))\nnum_ctr_ohe_feats = len(ctr_ohe_dict)\nprint num_ctr_ohe_feats\nprint ctr_ohe_dict[(0, '')]"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["###  Applying OHE to the dataset"],"metadata":{}},{"cell_type":"code","source":["ohe_dict_broadcast = sc.broadcast(ctr_ohe_dict)\nohe_dict_udf = ohe_udf_generator(ohe_dict_broadcast)\nohe_train_df = (parsed_train_df\n                  .select(parsed_train_df.label.alias('label'),ohe_dict_udf(parsed_train_df.feature).alias('features'))\n                  .cache()\n               )\n\nprint ohe_train_df.count()\nprint ohe_train_df.take(1)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Visualization: Feature frequency"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType, IntegerType\nfrom pyspark.sql.functions import log\n\nget_indices = udf(lambda sv: map(int, sv.indices), ArrayType(IntegerType()))\nfeature_counts = (ohe_train_df\n                   .select(explode(get_indices('features')))\n                   .groupBy('col')\n                   .count()\n                   .withColumn('bucket', log('count').cast('int'))\n                   .groupBy('bucket')\n                   .count()\n                   .orderBy('bucket')\n                   .collect())\nfeature_counts"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nx, y = zip(*feature_counts)\nx, y = x, np.log(y)\n\ndef prepare_plot(xticks, yticks, figsize=(10.5, 6), hide_labels=False, grid_color='#999999',\n                 grid_width=1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position('none')\n        axis.set_ticks(ticks)\n        axis.label.set_color('#999999')\n        if hide_labels: axis.set_ticklabels([])\n    plt.grid(color=grid_color, linewidth=grid_width, linestyle='-')\n    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n    return fig, ax\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(0, 12, 1), np.arange(0, 14, 2))\nax.set_xlabel(r'$\\log_e(bucketSize)$'), ax.set_ylabel(r'$\\log_e(countInBucket)$')\nplt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["###  Handling unseen features"],"metadata":{}},{"cell_type":"code","source":["def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n\n    Note:\n        You should ensure that the indices used to create a SparseVector are sorted, and that the\n        function handles missing features.\n\n    Args:\n        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n            (featureID, value) to unique integer.\n        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n            value).\n\n    Returns:\n        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n            identifiers for the (featureID, value) combinations that occur in the observation and\n            with values equal to 1.0.\n    \"\"\"\n    y=[]\n    z=0\n    for x in raw_feats:\n      if x in ohe_dict_broadcast.value:\n        y.append(ohe_dict_broadcast.value[x])\n        z+=1\n    y.sort()\n    return SparseVector(num_ohe_feats,y,np.ones(z))\n\nohe_dict_missing_udf = ohe_udf_generator(ohe_dict_broadcast)\nparsed_val=parse_raw_df(raw_validation_df)\nohe_validation_df = parsed_val.select(parsed_val.label.alias('label'),ohe_dict_missing_udf(parsed_val.feature).alias('features')).cache()\n\nohe_validation_df.count()\nohe_validation_df.show(1, truncate=False)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["##  CTR prediction and logloss evaluation"],"metadata":{}},{"cell_type":"markdown","source":["### Logistic regression"],"metadata":{}},{"cell_type":"code","source":["standardization = False\nelastic_net_param = 0.0\nreg_param = .01\nmax_iter = 20\n\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(regParam=reg_param,standardization=standardization,elasticNetParam=elastic_net_param,maxIter=max_iter)\n\nlr_model_basic = lr.fit(ohe_train_df)\n\nprint 'intercept: {0}'.format(lr_model_basic.intercept)\nprint 'length of coefficients: {0}'.format(len(lr_model_basic.coefficients))\nsorted_coefficients = sorted(lr_model_basic.coefficients)[:5]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql.functions import when, log, col\nepsilon = 1e-16\n\ndef add_log_loss(df):\n    \"\"\"Computes and adds a 'log_loss' column to a DataFrame using 'p' and 'label' columns.\n\n    Note:\n        log(0) is undefined, so when p is 0 we add a small value (epsilon) to it and when\n        p is 1 we subtract a small value (epsilon) from it.\n\n    Args:\n        df (DataFrame with 'p' and 'label' columns): A DataFrame with a probability column\n            'p' and a 'label' column that corresponds to y in the log loss formula.\n\n    Returns:\n        DataFrame: A new DataFrame with an additional column called 'log_loss' where 'log_loss' column contains the loss value as explained above.\n    \"\"\"\n    return df.select(df.p,df.label,when(df.p==0,epsilon).otherwise(when(df.p==1,log(df.p)-epsilon).otherwise(when(df.label==1,-log(df.p)).otherwise(when(df.label==0,-log(1-df.p))))).alias('log_loss'))\n\nadd_log_loss(example_log_loss_df).show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["###  Baseline log loss"],"metadata":{}},{"cell_type":"code","source":["\nfrom pyspark.sql.functions import lit\nclass_one_frac_train = ohe_train_df.groupBy().avg('label').first()[0]\nprint 'Training class one fraction = {0:.3f}'.format(class_one_frac_train)\n\nlog_loss_tr_base = add_log_loss(ohe_train_df.select(ohe_train_df.label.alias('label'),lit(class_one_frac_train).alias('p'))).groupBy().avg('log_loss').first()[0]\nprint 'Baseline Train Logloss = {0:.3f}\\n'.format(log_loss_tr_base)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["###  Predicted probability"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom math import exp #  exp(-t) = e^-t\n\ndef add_probability(df, model):\n    \"\"\"Adds a probability column ('p') to a DataFrame given a model\"\"\"\n    coefficients_broadcast = sc.broadcast(model.coefficients)\n    intercept = model.intercept\n\n    def get_p(features):\n        \"\"\"Calculate the probability for an observation given a list of features.\n\n        Note:\n            We'll bound our raw prediction between 20 and -20 for numerical purposes.\n\n        Args:\n            features: the features\n\n        Returns:\n            float: A probability between 0 and 1.\n        \"\"\"\n        # Compute the raw value\n        raw_prediction = intercept+coefficients_broadcast.value.dot(features)\n        # Bound the raw value between 20 and -20\n        raw_prediction = 20 if raw_prediction > 20 else -20 if raw_prediction < -20 else raw_prediction\n        # Return the probability\n        return (1+exp(-raw_prediction))**(-1)\n\n    get_p_udf = udf(get_p, DoubleType())\n    return df.withColumn('p', get_p_udf('features'))\n\nadd_probability_model_basic = lambda df: add_probability(df, lr_model_basic)\ntraining_predictions = add_probability_model_basic(ohe_train_df).cache()\n\ntraining_predictions.show(5)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["###  Evaluating the model"],"metadata":{}},{"cell_type":"code","source":["def evaluate_results(df, model, baseline=None):\n    \"\"\"Calculates the log loss for the data given the model.\n\n    Note:\n        If baseline has a value the probability should be set to baseline before\n        the log loss is calculated.  Otherwise, use add_probability to add the\n        appropriate probabilities to the DataFrame.\n\n    Args:\n        df (DataFrame with 'label' and 'features' columns): A DataFrame containing\n            labels and features.\n        model (LogisticRegressionModel): A trained logistic regression model. This\n            can be None if baseline is set.\n        baseline (float): A baseline probability to use for the log loss calculation.\n\n    Returns:\n        float: Log loss for the data.\n    \"\"\"\n    if (baseline):\n      with_probability_df = df.withColumn('p', lit(baseline))\n    else:\n        with_probability_df = add_probability(df, model)\n    \n    with_log_loss_df = add_log_loss(with_probability_df)\n    log_loss = with_log_loss_df.groupBy().avg('log_loss').first()[0]\n    return log_loss\n\nlog_loss_train_model_basic = evaluate_results(ohe_train_df, lr_model_basic)\nprint ('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n       .format(log_loss_tr_base, log_loss_train_model_basic))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["###  Validation log loss\n\nNext, using the `evaluate_results` function compute the validation log loss for both the baseline and logistic regression models. Notably, the baseline model for the validation data should still be based on the label fraction from the training dataset."],"metadata":{}},{"cell_type":"code","source":["log_loss_val_base = evaluate_results(ohe_validation_df,lr_model_basic,baseline=class_one_frac_train)\n\nlog_loss_val_l_r0 = evaluate_results(ohe_validation_df,lr_model_basic)\nprint ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n       .format(log_loss_val_base, log_loss_val_l_r0))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Visualization: ROC curve"],"metadata":{}},{"cell_type":"code","source":["labels_and_scores = add_probability_model_basic(ohe_validation_df).select('label', 'p')\nlabels_and_weights = labels_and_scores.collect()\nlabels_and_weights.sort(key=lambda (k, v): v, reverse=True)\nlabels_by_weight = np.array([k for (k, v) in labels_and_weights])\n\nlength = labels_by_weight.size\ntrue_positives = labels_by_weight.cumsum()\nnum_positive = true_positives[-1]\nfalse_positives = np.arange(1.0, length + 1, 1.) - true_positives\n\ntrue_positive_rate = true_positives / num_positive\nfalse_positive_rate = false_positives / (length - num_positive)\n\n# Generate layout and plot data\nfig, ax = prepare_plot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\nax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\nax.set_ylabel('True Positive Rate (Sensitivity)')\nax.set_xlabel('False Positive Rate (1 - Specificity)')\nplt.plot(false_positive_rate, true_positive_rate, color='#8cbfd0', linestyle='-', linewidth=3.)\nplt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["## Reducing feature dimension via feature hashing"],"metadata":{}},{"cell_type":"code","source":["from collections import defaultdict\nimport hashlib\n\ndef hash_function(raw_feats, num_buckets, print_mapping=False):\n    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n\n    Note:\n        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n\n    Args:\n        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n            (featureID, value) tuples.\n        num_buckets (int): Number of buckets to use as features.\n        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n            printed.\n\n    Returns:\n        dict of int to float:  The keys will be integers which represent the buckets that the\n            features have been hashed to.  The value for a given key will contain the count of the\n            (featureID, value) tuples that have hashed to that key.\n    \"\"\"\n    mapping = { category + ':' + str(ind):\n                int(int(hashlib.md5(category + ':' + str(ind)).hexdigest(), 16) % num_buckets)\n                for ind, category in raw_feats}\n    if(print_mapping): print mapping\n\n    def map_update(l, r):\n        l[r] += 1.0\n        return l\n\n    sparse_features = reduce(map_update, mapping.values(), defaultdict(float))\n    return dict(sparse_features)\n\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["###  Creating hashed features"],"metadata":{}},{"cell_type":"code","source":["parsed_train_df.first()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vectors\nnum_hash_buckets = 2 ** 15\n\n# UDF that returns a vector of hashed features given an Array of tuples\ntuples_to_hash_features_udf = udf(lambda x: Vectors.sparse(num_hash_buckets, hash_function(x, num_hash_buckets)), VectorUDT())\n\ndef add_hashed_features(df):\n    \"\"\"Return a DataFrame with labels and hashed features.\n\n    Note:\n        Make sure to cache the DataFrame that you are returning.\n\n    Args:\n        df (DataFrame with 'tuples' column): A DataFrame containing the tuples to be hashed.\n\n    Returns:\n        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n            SparseVector of hashed features.\n    \"\"\"\n    return df.select(df.label,tuples_to_hash_features_udf(df.feature).alias('features')).cache()\n\nhash_train_df = add_hashed_features(parsed_train_df)\nhash_validation_df = add_hashed_features(parsed_val)\nparsed_test_df=parse_raw_df(raw_test_df)\nhash_test_df = add_hashed_features(parsed_test_df)\n\nhash_train_df.show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["###  Sparsity"],"metadata":{}},{"cell_type":"code","source":["def vector_feature_sparsity(sparse_vector):\n    \"\"\"Calculates the sparsity of a SparseVector.\n\n    Args:\n        sparse_vector (SparseVector): The vector containing the features.\n\n    Returns:\n        float: The ratio of features found in the vector to the total number of features.\n    \"\"\"\n    return float(len(sparse_vector.values))/float(len(sparse_vector))\n\nfeature_sparsity_udf = udf(vector_feature_sparsity, DoubleType())\n\na_sparse_vector = Vectors.sparse(5, {0: 1.0, 3: 1.0})\na_sparse_vector_sparsity = vector_feature_sparsity(a_sparse_vector)\nprint 'This vector should have sparsity 2/5 or .4.'\nprint 'Sparsity = {0:.2f}.'.format(a_sparse_vector_sparsity)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["feature_sparsity_udf = udf(vector_feature_sparsity, DoubleType())\n\ndef get_sparsity(df):\n    \"\"\"Calculates the average sparsity for the features in a DataFrame.\n\n    Args:\n        df (DataFrame with 'features' column): A DataFrame with sparse features.\n\n    Returns:\n        float: The average feature sparsity.\n    \"\"\"\n    return df.select(feature_sparsity_udf(df.features).alias('avga')).groupBy().avg('avga').first()[0]\n\naverage_sparsity_ohe = get_sparsity(ohe_train_df)\naverage_sparsity_hash = get_sparsity(hash_train_df)\n\nprint 'Average OHE Sparsity: {0:.7e}'.format(average_sparsity_ohe)\nprint 'Average Hash Sparsity: {0:.7e}'.format(average_sparsity_hash)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["###  Logistic model with hashed features"],"metadata":{}},{"cell_type":"code","source":["standardization = False\nelastic_net_param = 0.7\nreg_param = .001\nmax_iter = 20\n\nlr_hash = LogisticRegression(elasticNetParam=elastic_net_param,maxIter=max_iter,regParam=reg_param,standardization=standardization)\n\nlr_model_hashed = lr_hash.fit(hash_train_df)\nprint 'intercept: {0}'.format(lr_model_hashed.intercept)\nprint len(lr_model_hashed.coefficients)\n\nlog_loss_train_model_hashed = evaluate_results(hash_train_df,lr_model_hashed)\nprint ('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\thashed = {1:.3f}'\n       .format(log_loss_tr_base, log_loss_train_model_hashed))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["###  Evaluating on the test set"],"metadata":{}},{"cell_type":"code","source":["log_loss_test = evaluate_results(hash_test_df,lr_model_hashed)\n\n# Log loss for the baseline model\nclass_one_frac_test = hash_test_df.groupBy().avg('label').first()[0]\nprint 'Class one fraction for test data: {0}'.format(class_one_frac_test)\nlog_loss_test_baseline = evaluate_results(hash_test_df,lr_model_basic,baseline=class_one_frac_test)\n\nprint ('Hashed Features Test Log Loss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n       .format(log_loss_test_baseline, log_loss_test))"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["##  Cleanup"],"metadata":{}},{"cell_type":"code","source":["downloaded_data_file = None\nraw_df = None\ncleanup_old_downloads()"],"metadata":{},"outputs":[],"execution_count":54}],"metadata":{"name":"cs120_lab3_ctr_df","notebookId":3623611354004650},"nbformat":4,"nbformat_minor":0}
